{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, EarlyStoppingCallback\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n"
      ],
      "metadata": {
        "id": "QJXqHxvOVDKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79TtiE58LUsW"
      },
      "outputs": [],
      "source": [
        "dev_test_list = [\"dev_test_it.tsv\", \"dev_test_en.tsv\", \"dev_test_de.tsv\", \"dev_test_ar.tsv\", \"dev_test_bg.tsv\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_monolingual_subjectivity_classifier(model_name, train_data, val_data, learning_rate, epoch, weight_decay=0.05):\n",
        "    # Load tokenizer using AutoTokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = SubjectivityDataset(train_data, tokenizer, MAX_LENGTH)\n",
        "    val_dataset = SubjectivityDataset(val_data, tokenizer, MAX_LENGTH)\n",
        "\n",
        "    # Use AutoModelForSequenceClassification instead of RobertaForSequenceClassification\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "        ignore_mismatched_sizes=True  # For binary classification (subjective/objective)\n",
        "    )\n",
        "\n",
        "    class_weights = torch.tensor([1.5, 0.5]).to(device)  # Give more weight to SUBJ class\n",
        "    #loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Define training arguments with compatibility for older transformers versions\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        eval_strategy=\"epoch\",  # Changed from evaluation_strategy\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        num_train_epochs=epoch,\n",
        "        weight_decay= weight_decay,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        save_strategy=\"epoch\",  # This might need to be adjusted\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=100,\n",
        "        logging_strategy=\"epoch\",  # This might need to be changed to logging_mode\n",
        "        report_to='none'\n",
        "    )\n",
        "    early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.001\n",
        ")\n",
        "\n",
        "    # If the above fails due to older transformers version, try this alternate version:\n",
        "    # training_args = TrainingArguments(\n",
        "    #     output_dir=\"./results\",\n",
        "    #     eval_steps=500,\n",
        "    #     learning_rate=LEARNING_RATE,\n",
        "    #     per_device_train_batch_size=BATCH_SIZE,\n",
        "    #     per_device_eval_batch_size=BATCH_SIZE,\n",
        "    #     num_train_epochs=EPOCHS,\n",
        "    #     weight_decay=0.01,\n",
        "    #     logging_dir=\"./logs\",\n",
        "    #     logging_steps=100,\n",
        "    #     report_to=None\n",
        "    # )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks = [early_stopping_callback],\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    eval_result = trainer.evaluate()\n",
        "    print(f\"Evaluation results: {eval_result}\")\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "xnD9dVVFU_lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_paths):\n",
        "    \"\"\"Load data from multiple TSV files and combine them.\"\"\"\n",
        "    dfs = []\n",
        "    for file_path in file_paths:\n",
        "        df = pd.read_csv(file_path, sep='\\t', header=0)\n",
        "        # Add language tag based on filename\n",
        "        language = os.path.basename(file_path).split('_')[1].split('.')[0]\n",
        "        df['language'] = language\n",
        "        dfs.append(df)\n",
        "\n",
        "    return pd.concat(dfs, ignore_index=True)"
      ],
      "metadata": {
        "id": "CuNRtAs5WLgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a Dataset class\n",
        "class SubjectivityDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.data.iloc[idx]['sentence']\n",
        "        label = self.data.iloc[idx]['label_id']\n",
        "\n",
        "        # Tokenize the sentence\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Remove the batch dimension added by the tokenizer\n",
        "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "\n",
        "        # Add the label\n",
        "        encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return encoding\n"
      ],
      "metadata": {
        "id": "H7Tj0GO-WZdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average='weighted', zero_division=0\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n"
      ],
      "metadata": {
        "id": "uqZDbrU1WbHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dataset class for test data\n",
        "class TestSubjectivityDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.data.iloc[idx]['sentence']\n",
        "        label = self.data.iloc[idx]['label_id']\n",
        "\n",
        "        # Tokenize the sentence\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Remove the batch dimension\n",
        "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "\n",
        "        # Add the label and sentence ID\n",
        "        encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "        encoding['sentence_idx'] = idx\n",
        "\n",
        "        return encoding"
      ],
      "metadata": {
        "id": "IkTSi16ZWiCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_test_set(test_file_path, model, tokenizer, filename, max_length=128, batch_size=16):\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(test_file_path, sep='\\t')\n",
        "    print(f\"Loaded test data with {len(test_data)} examples\")\n",
        "    print(f\"Columns: {test_data.columns.tolist()}\")\n",
        "\n",
        "    # Map labels to IDs\n",
        "    test_data['label_id'] = test_data['label'].map({'OBJ': 0, 'SUBJ': 1})\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    test_dataset = TestSubjectivityDataset(test_data, tokenizer, max_length)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Lists to store results\n",
        "    all_predictions = []\n",
        "    all_pred_labels = []\n",
        "    all_true_labels = []\n",
        "    all_indices = []\n",
        "\n",
        "    # Perform predictions\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            # Get the sentence indices\n",
        "            indices = batch.pop('sentence_idx')\n",
        "\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items() if k != 'sentence_idx'}\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = model(**batch)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            pred_classes = torch.argmax(predictions, dim=1)\n",
        "\n",
        "            # Store results\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_pred_labels.extend(pred_classes.cpu().numpy())\n",
        "            all_true_labels.extend(batch['labels'].cpu().numpy())\n",
        "            all_indices.extend(indices.numpy())\n",
        "\n",
        "    # Convert to text labels\n",
        "    pred_text_labels = [\"OBJ\" if p == 0 else \"SUBJ\" for p in all_pred_labels]\n",
        "    true_text_labels = [\"OBJ\" if t == 0 else \"SUBJ\" for t in all_true_labels]\n",
        "\n",
        "    # Create results dataframe\n",
        "    results_df = pd.DataFrame({\n",
        "        'sentence_id': [test_data.iloc[idx]['sentence_id'] for idx in all_indices],\n",
        "        'sentence': [test_data.iloc[idx]['sentence'] for idx in all_indices],\n",
        "        'true_label': true_text_labels,\n",
        "        'predicted_label': pred_text_labels,\n",
        "        'obj_score': [round(p[0], 4) for p in all_predictions],\n",
        "        'subj_score': [round(p[1], 4) for p in all_predictions],\n",
        "    })\n",
        "\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_true_labels, all_pred_labels, average='macro'\n",
        "    )\n",
        "\n",
        "\n",
        "    # ========== NEW CODE FOR SUBMISSION FILE ==========\n",
        "    # Create submission dataframe with required format\n",
        "    submission_df = results_df[['sentence_id', 'predicted_label']].copy()\n",
        "    submission_df.columns = ['sentence_id', 'label']  # Rename columns\n",
        "\n",
        "    # Save to TSV\n",
        "    submission_output_path = filename\n",
        "    submission_df.to_csv(submission_output_path, sep='\\t', index=False)\n",
        "    print(f\"\\nSubmission file saved to {submission_output_path}\")\n",
        "    # ========== END NEW CODE ==========\n",
        "\n",
        "    # [The rest of the original code remains unchanged...]\n",
        "\n",
        "\n",
        "    # Detailed report\n",
        "    class_report = classification_report(all_true_labels, all_pred_labels,\n",
        "                                         target_names=['OBJ', 'SUBJ'], output_dict=True)\n",
        "\n",
        "    # Print detailed metrics\n",
        "    print(f\"\\n===== Model Performance on {test_file_path} =====\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score (weighted): {f1:.4f}\")\n",
        "    print(f\"Precision (weighted): {precision:.4f}\")\n",
        "    print(f\"Recall (weighted): {recall:.4f}\\n\")\n",
        "\n",
        "    # Print per-class metrics\n",
        "    print(\"Class-wise Performance:\")\n",
        "    print(f\"OBJ - Precision: {class_report['OBJ']['precision']:.4f}, \"\n",
        "          f\"Recall: {class_report['OBJ']['recall']:.4f}, \"\n",
        "          f\"F1: {class_report['OBJ']['f1-score']:.4f}\")\n",
        "    print(f\"SUBJ - Precision: {class_report['SUBJ']['precision']:.4f}, \"\n",
        "          f\"Recall: {class_report['SUBJ']['recall']:.4f}, \"\n",
        "          f\"F1: {class_report['SUBJ']['f1-score']:.4f}\")\n",
        "    print(f\"-----macro avg F1-------- {(class_report['OBJ']['f1-score']+class_report['SUBJ']['f1-score'])/2}\" )\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_true_labels, all_pred_labels)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(\"              Predicted\")\n",
        "    print(\"             OBJ    SUBJ\")\n",
        "    print(f\"Actual OBJ  {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
        "    print(f\"      SUBJ  {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
        "\n",
        "    # Save results\n",
        "    output_path = f\"predictions_{test_file_path.split('/')[-1]}\"\n",
        "    results_df.to_csv(output_path, sep='\\t', index=False)\n",
        "    print(f\"\\nDetailed predictions saved to {output_path}\")\n",
        "\n",
        "    # Error analysis - find examples where model was wrong\n",
        "    errors_df = results_df[results_df['true_label'] != results_df['predicted_label']]\n",
        "    if not errors_df.empty:\n",
        "        error_output_path = f\"errors_{test_file_path.split('/')[-1]}\"\n",
        "        errors_df.to_csv(error_output_path, sep='\\t', index=False)\n",
        "        print(f\"Examples of misclassifications saved to {error_output_path}\")\n",
        "\n",
        "        # Print a few examples of misclassifications\n",
        "        print(\"\\nExamples of misclassifications:\")\n",
        "        sample_errors = errors_df.sample(min(5, len(errors_df)))\n",
        "        for _, row in sample_errors.iterrows():\n",
        "            print(f\"Sentence ID: {row['sentence_id']}\")\n",
        "            print(f\"Sentence: {row['sentence']}\")\n",
        "            print(f\"True: {row['true_label']}, Predicted: {row['predicted_label']}\")\n",
        "            print(f\"Confidence scores - OBJ: {row['obj_score']}, SUBJ: {row['subj_score']}\")\n",
        "            print(\"\")\n",
        "\n",
        "    return results_df, {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'class_report': class_report\n",
        "    }"
      ],
      "metadata": {
        "id": "5niFXiiOWkHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkWbAteKGg5u"
      },
      "source": [
        "## English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h44IgGpHmgL"
      },
      "outputs": [],
      "source": [
        "# 1. Set up constants\n",
        "ENGLISH_MODEL =  \"cardiffnlp/twitter-roberta-base-sentiment\" #\"microsoft/deberta-v3-base\" #distilroberta-base\" #\"lighteternal/fact-or-opinion-xlmr-el\" #\"meta-llama/Llama-Prompt-Guard-2-22M\" #\"Elron/bleurt-tiny-512\" #\"cardiffnlp/twitter-xlm-roberta-base-sentiment\" #\"FacebookAI/roberta-base\"   #\"textattack/albert-base-v2-imdb\" #\"Elron/bleurt-tiny-512\" #\"philschmid/tiny-bert-sst2-distilled\"#\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-docnli-ling-2c\" #\"oeg/BERT-Repository-Proposal\" #\"FacebookAI/roberta-base\"\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 7\n",
        "LEARNING_RATE = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4PhDEWkHyxH"
      },
      "outputs": [],
      "source": [
        "# Load training and validation data separately\n",
        "train_data = load_data(['train_en.tsv'])\n",
        "val_data = load_data(['dev_en.tsv'])\n",
        "\n",
        "# Convert labels to integers\n",
        "label_map = {'OBJ': 0, 'SUBJ': 1}\n",
        "train_data['label_id'] = train_data['label'].map(label_map)\n",
        "val_data['label_id'] = val_data['label'].map(label_map)\n",
        "\n",
        "# Reset indices\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "val_data = val_data.reset_index(drop=True)\n",
        "\n",
        "print(f\"Training with {len(train_data)} examples\")\n",
        "print(f\"Validating with {len(val_data)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aIKCnCMEIcK0"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model, tokenizer = train_monolingual_subjectivity_classifier_it(ENGLISH_MODEL, train_data, val_data, 2e-5, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dRicXxPQnf5"
      },
      "outputs": [],
      "source": [
        "test_file = test_list[1]\n",
        "results, metrics = evaluate_on_test_set(test_file, model, tokenizer)\n",
        "print(\"\\nEvaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYmEipJmJwVC"
      },
      "source": [
        "## Italian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1bNzXRQIqPL"
      },
      "outputs": [],
      "source": [
        "ITALIAN_MODEL =  \"neuraly/bert-base-italian-cased-sentiment\" # \"dbmdz/bert-base-italian-cased\" #\"Musixmatch/umberto-commoncrawl-cased-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he8P7W7bLp0Q"
      },
      "outputs": [],
      "source": [
        "# Load training and validation data separately\n",
        "train_data_it = load_data([\"train_it.tsv\"])\n",
        "val_data_it = load_data([\"dev_it.tsv\"])\n",
        "\n",
        "# Convert labels to integers\n",
        "label_map = {'OBJ': 0, 'SUBJ': 1}\n",
        "train_data_it['label_id'] = train_data_it['label'].map(label_map)\n",
        "val_data_it['label_id'] = val_data_it['label'].map(label_map)\n",
        "\n",
        "# Reset indices\n",
        "train_data_it = train_data_it.reset_index(drop=True)\n",
        "val_data_it = val_data_it.reset_index(drop=True)\n",
        "\n",
        "print(f\"Training with {len(train_data_it)} examples\")\n",
        "print(f\"Validating with {len(val_data_it)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nNwS9EFPGDx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ltgm8AEMx2w"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model, tokenizer = train_monolingual_subjectivity_classifier_it(ITALIAN_MODEL, train_data_it, val_data_it, 2e-5,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLQMQd_5Mm54"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJK54C3LQt6I"
      },
      "outputs": [],
      "source": [
        "test_file = test_list[0]\n",
        "results, metrics = evaluate_on_test_set(test_file, model, tokenizer)\n",
        "print(\"\\nEvaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGZv-2pdMrmV"
      },
      "source": [
        "# Arabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHR-WUSnMl63"
      },
      "outputs": [],
      "source": [
        "ARABIC_MODEL = \"omarelshehy/Arabic-Retrieval-v1.0\" #\"CAMeL-Lab/bert-base-arabic-camelbert-mix\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVNKxpnNNHVt"
      },
      "outputs": [],
      "source": [
        "# Load training and validation data separately\n",
        "train_data_ar = load_data([\"train_ar.tsv\"])\n",
        "val_data_ar = load_data([\"dev_ar.tsv\"])\n",
        "\n",
        "# Convert labels to integers\n",
        "label_map = {'OBJ': 0, 'SUBJ': 1}\n",
        "train_data_ar['label_id'] = train_data_ar['label'].map(label_map)\n",
        "val_data_ar['label_id'] = val_data_ar['label'].map(label_map)\n",
        "\n",
        "# Reset indices\n",
        "train_data_ar = train_data_ar.reset_index(drop=True)\n",
        "val_data_ar = val_data_ar.reset_index(drop=True)\n",
        "\n",
        "print(f\"Training with {len(train_data_ar)} examples\")\n",
        "print(f\"Validating with {len(val_data_ar)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8LUneP0M79a"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model, tokenizer = train_monolingual_subjectivity_classifier_it(ARABIC_MODEL, train_data_ar, val_data_ar, 1e-5,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4UqyWbbQ6B-"
      },
      "outputs": [],
      "source": [
        "test_file = test_list[3]\n",
        "results, metrics = evaluate_on_test_set(test_file, model, tokenizer)\n",
        "print(\"\\nEvaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# German\n"
      ],
      "metadata": {
        "id": "Lvwxw4nUVH2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GERMAN_MODEL = \"ssary/XLM-RoBERTa-German-sentiment\""
      ],
      "metadata": {
        "id": "YwhTZdB2YJ8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training and validation data separately\n",
        "train_data_bg = load_data([\"train_bg.tsv\"])\n",
        "val_data_bg = load_data([\"dev_bg.tsv\"])\n",
        "\n",
        "# Convert labels to integers\n",
        "label_map = {'OBJ': 0, 'SUBJ': 1}\n",
        "train_data_bg['label_id'] = train_data_bg['label'].map(label_map)\n",
        "val_data_bg['label_id'] = val_data_bg['label'].map(label_map)\n",
        "\n",
        "# Reset indices\n",
        "train_data_bg = train_data_bg.reset_index(drop=True)\n",
        "val_data_bg = val_data_bg.reset_index(drop=True)\n",
        "\n",
        "print(f\"Training with {len(train_data_bg)} examples\")\n",
        "print(f\"Validating with {len(val_data_bg)} examples\")\n"
      ],
      "metadata": {
        "id": "--KvrSarYjMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model, tokenizer = train_monolingual_subjectivity_classifier(GERMAN_MODEL, train_data_de, val_data_de, 2e-5,5)"
      ],
      "metadata": {
        "id": "fyJIthzUYk5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results, metrics = evaluate_on_test_set(\"dev_test_de.tsv\", model, tokenizer, \"subtask_german.tsv\")\n",
        "print(\"\\nEvaluation complete!\")"
      ],
      "metadata": {
        "id": "vueMoK82Ymaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results, metrics = evaluate_on_test_set(\"test_de_unlabeled.tsv\", model, tokenizer, \"subtask_german.tsv\")\n",
        "print(\"\\nEvaluation complete!\")"
      ],
      "metadata": {
        "id": "osWztSDvZFYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bulgarian\n"
      ],
      "metadata": {
        "id": "qOmKqXHbVKeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BULGARIAN_MODEL = \"ankitkupadhyay/xnli3.0_bulgarian_model\""
      ],
      "metadata": {
        "id": "k4UZH997VScE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training and validation data separately\n",
        "train_data_bg = load_data([\"train_bg.tsv\"])\n",
        "val_data_bg = load_data([\"dev_bg.tsv\"])\n",
        "\n",
        "# Convert labels to integers\n",
        "label_map = {'OBJ': 0, 'SUBJ': 1}\n",
        "train_data_bg['label_id'] = train_data_bg['label'].map(label_map)\n",
        "val_data_bg['label_id'] = val_data_bg['label'].map(label_map)\n",
        "\n",
        "# Reset indices\n",
        "train_data_bg = train_data_bg.reset_index(drop=True)\n",
        "val_data_bg = val_data_bg.reset_index(drop=True)\n",
        "\n",
        "print(f\"Training with {len(train_data_bg)} examples\")\n",
        "print(f\"Validating with {len(val_data_bg)} examples\")\n"
      ],
      "metadata": {
        "id": "oSaO1STnVTLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model, tokenizer = train_monolingual_subjectivity_classifier(BULGARIAN_MODEL, train_data_bg, val_data_bg, 2e-5,3, 0.01)"
      ],
      "metadata": {
        "id": "PIm0BwdtVTZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results, metrics = evaluate_on_test_set(\"dev_test_bg.tsv\", model, tokenizer, \"subtask_bulgarian.tsv\")\n",
        "print(\"\\nEvaluation complete!\")"
      ],
      "metadata": {
        "id": "-RNaD_dTXc-B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}