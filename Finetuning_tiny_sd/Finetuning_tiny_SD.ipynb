{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import libraries and setup\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from accelerate import Accelerator\n",
        "from tqdm import tqdm\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import logging\n",
        "#\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check GPU\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8W2IwCFA5lgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 3: Configuration (Modify these parameters as needed)\n",
        "class Config:\n",
        "    model_name = \"segmind/tiny-sd\"\n",
        "    images_dir = \"dataset/images\"  # Your images folder\n",
        "    captions_csv = \"dataset/captions.csv\"  # Your captions file\n",
        "    output_dir = \"./fine_tuned_tiny_sd\"\n",
        "    resolution = 512\n",
        "    batch_size = 2  # Reduced for Colab\n",
        "    num_epochs = 15  # More epochs for small dataset\n",
        "    learning_rate = 5e-5  # Lower learning rate\n",
        "    lora_rank = 16  # Higher rank for more capacity\n",
        "    batch_size = 1  # Smaller batches\n",
        "    lora_alpha = 32\n",
        "    gradient_accumulation_steps = 4\n",
        "    mixed_precision = \"no\"\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "YzIkT7n25tj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images_dir, captions_csv, tokenizer, size=512):\n",
        "        self.images_dir = images_dir\n",
        "        self.size = size\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Load captions\n",
        "        self.captions_df = pd.read_csv(captions_csv)\n",
        "        # Assuming CSV has columns: 'image_filename', 'caption'\n",
        "        self.image_files = self.captions_df['filename'].tolist()\n",
        "        self.captions = self.captions_df[' captions'].tolist()\n",
        "\n",
        "        print(f\"Loaded {len(self.image_files)} images\")\n",
        "\n",
        "        # Image preprocessing\n",
        "        self.image_transforms = transforms.Compose([\n",
        "            transforms.Resize((size, size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        image_path = os.path.join(self.images_dir, self.image_files[idx])\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            image = self.image_transforms(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            # Return a blank image if loading fails\n",
        "            image = torch.zeros(3, self.size, self.size)\n",
        "\n",
        "        # Tokenize caption\n",
        "        caption = str(self.captions[idx])\n",
        "        text_inputs = self.tokenizer(\n",
        "            caption,\n",
        "            padding=\"max_length\",\n",
        "            max_length=77,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": image,\n",
        "            \"input_ids\": text_inputs.input_ids.squeeze(),\n",
        "            \"attention_mask\": text_inputs.attention_mask.squeeze()\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
        "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
        "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask\n",
        "    }\n"
      ],
      "metadata": {
        "id": "cC6-IjaL508g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading model...\")\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    config.model_name,\n",
        "    torch_dtype=torch.float32  # Changed to float32 to avoid dtype issues\n",
        ")\n",
        "\n",
        "# Extract components\n",
        "unet = pipe.unet.to(torch.float32)  # Ensure float32\n",
        "text_encoder = pipe.text_encoder.to(torch.float32)  # Ensure float32\n",
        "tokenizer = pipe.tokenizer\n",
        "vae = pipe.vae.to(torch.float32)  # Ensure float32\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(config.model_name, subfolder=\"scheduler\")\n",
        "\n",
        "# Freeze VAE and text encoder\n",
        "vae.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)\n",
        "\n",
        "# Setup LoRA for UNet\n",
        "lora_config = LoraConfig(\n",
        "    r=config.lora_rank,\n",
        "    lora_alpha=config.lora_alpha,\n",
        "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
        "    lora_dropout=0.1,\n",
        "    # Remove task_type as it's not needed for diffusion models\n",
        ")\n",
        "\n",
        "unet = get_peft_model(unet, lora_config)\n",
        "unet.train()\n",
        "\n",
        "print(\"LoRA setup complete!\")"
      ],
      "metadata": {
        "id": "dy6NXxv85_UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Create dataset and dataloader\n",
        "print(\"Creating dataset...\")\n",
        "\n",
        "# Check if files exist\n",
        "if not os.path.exists(config.images_dir):\n",
        "    print(f\"Warning: {config.images_dir} directory not found!\")\n",
        "if not os.path.exists(config.captions_csv):\n",
        "    print(f\"Warning: {config.captions_csv} file not found!\")\n",
        "\n",
        "dataset = CustomDataset(\n",
        "    images_dir=config.images_dir,\n",
        "    captions_csv=config.captions_csv,\n",
        "    tokenizer=tokenizer,\n",
        "    size=config.resolution\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2  # Reduced for Colab\n",
        ")\n",
        "\n",
        "print(f\"Dataset created with {len(dataset)} samples\")"
      ],
      "metadata": {
        "id": "2e49bdNS6EZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Setup training\n",
        "# Initialize accelerator\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "    mixed_precision=config.mixed_precision\n",
        ")\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    unet.parameters(),\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Prepare everything with accelerator\n",
        "unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n",
        "\n",
        "# Move models to device\n",
        "vae = vae.to(accelerator.device)\n",
        "text_encoder = text_encoder.to(accelerator.device)\n",
        "\n",
        "print(\"Training setup complete!\")\n"
      ],
      "metadata": {
        "id": "BIrHGQWw6IDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Training loop\n",
        "print(\"Starting training...\")\n",
        "global_step = 0\n",
        "os.makedirs(config.output_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(config.num_epochs):\n",
        "    unet.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        with accelerator.accumulate(unet):\n",
        "            # Convert images to latent space\n",
        "            with torch.no_grad():\n",
        "                latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n",
        "                latents = latents * vae.config.scaling_factor\n",
        "\n",
        "            # Add noise to latents\n",
        "            noise = torch.randn_like(latents)\n",
        "            timesteps = torch.randint(\n",
        "                0, noise_scheduler.config.num_train_timesteps,\n",
        "                (latents.shape[0],), device=latents.device\n",
        "            ).long()\n",
        "\n",
        "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # Get text embeddings\n",
        "            with torch.no_grad():\n",
        "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "            # Predict noise\n",
        "            noise_pred = unet(\n",
        "                noisy_latents,\n",
        "                timesteps,\n",
        "                encoder_hidden_states\n",
        "            ).sample\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
        "\n",
        "            # Backward pass\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'avg_loss': f'{epoch_loss/(batch_idx+1):.4f}'\n",
        "            })\n",
        "\n",
        "    avg_loss = epoch_loss/len(dataloader)\n",
        "    print(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint every epoch\n",
        "    if accelerator.is_main_process:\n",
        "        save_path = os.path.join(config.output_dir, f\"checkpoint-epoch-{epoch+1}\")\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        # Save LoRA weights\n",
        "        accelerator.unwrap_model(unet).save_pretrained(save_path)\n",
        "        print(f\"Checkpoint saved to {save_path}\")\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "dOXvTjC26K0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 9: Save final model\n",
        "print(\"Saving final model...\")\n",
        "if accelerator.is_main_process:\n",
        "    final_save_path = os.path.join(config.output_dir, \"final_model\")\n",
        "    os.makedirs(final_save_path, exist_ok=True)\n",
        "\n",
        "    # Save LoRA weights\n",
        "    accelerator.unwrap_model(unet).save_pretrained(final_save_path)\n",
        "\n",
        "    # Merge LoRA weights and save full pipeline\n",
        "    print(\"Merging LoRA weights...\")\n",
        "    unet_merged = accelerator.unwrap_model(unet).merge_and_unload()\n",
        "    pipe.unet = unet_merged.to(torch.float16)\n",
        "    pipe.save_pretrained(final_save_path)\n",
        "\n",
        "    print(f\"Final model saved to {final_save_path}\")"
      ],
      "metadata": {
        "id": "pQB8NIIu6PEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Test the fine-tuned model\n",
        "def test_model(prompt=\"a beautiful landscape\", save_name=\"test_output.png\"):\n",
        "    \"\"\"Test the fine-tuned model\"\"\"\n",
        "    print(f\"Generating image for prompt: '{prompt}'\")\n",
        "\n",
        "    # Load the fine-tuned model\n",
        "    test_pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        os.path.join(config.output_dir, \"final_model\"),\n",
        "        torch_dtype=torch.float32\n",
        "    )\n",
        "    test_pipe = test_pipe.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Generate image\n",
        "    with torch.no_grad():\n",
        "        image = test_pipe(\n",
        "            prompt,\n",
        "            num_inference_steps=50,\n",
        "            guidance_scale=7.5,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "    image.save(save_name)\n",
        "    print(f\"Test image saved as {save_name}\")\n",
        "    return image\n",
        "\n",
        "# Test the model (run this after training completes)\n",
        "test_image = test_model(\"Turab Hussain Usmani\")"
      ],
      "metadata": {
        "id": "jX_gINfO6QrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 11: Download the trained model (Optional)\n",
        "# Uncomment and run this to download your trained model\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create a zip of the final model\n",
        "shutil.make_archive('fine_tuned_tiny_sd', 'zip', config.output_dir)\n",
        "files.download('fine_tuned_tiny_sd.zip')\n",
        "\"\"\"\n",
        "\n",
        "print(\"Setup complete! Run the cells in order to fine-tune your model.\")"
      ],
      "metadata": {
        "id": "eE93IQZo6Ujf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}