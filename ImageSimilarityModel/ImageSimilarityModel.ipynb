{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX-KHYVv4J6Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from typing import Tuple, Union\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PerceptualFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature extractor based on ResNet18 with additional perceptual layers\n",
        "    to capture structural information while being robust to lighting changes.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(PerceptualFeatureExtractor, self).__init__()\n",
        "\n",
        "        # Use pre-trained ResNet18 as backbone\n",
        "        if pretrained:\n",
        "            self.backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        else:\n",
        "            self.backbone = resnet18(weights=None)\n",
        "\n",
        "        # Remove the final classification layer\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
        "\n",
        "        # Add custom layers for perceptual features\n",
        "        self.perceptual_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64)\n",
        "        )\n",
        "\n",
        "        # Gradient-based edge detector (Sobel-like)\n",
        "        self.register_buffer('sobel_x', torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3))\n",
        "        self.register_buffer('sobel_y', torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3))\n",
        "\n",
        "    def extract_edge_features(self, x):\n",
        "        \"\"\"Extract edge features that are robust to lighting changes\"\"\"\n",
        "        # Convert to grayscale\n",
        "        gray = torch.mean(x, dim=1, keepdim=True)\n",
        "\n",
        "        # Apply Sobel filters\n",
        "        edges_x = F.conv2d(gray, self.sobel_x, padding=1)\n",
        "        edges_y = F.conv2d(gray, self.sobel_y, padding=1)\n",
        "\n",
        "        # Combine edge information\n",
        "        edges = torch.sqrt(edges_x**2 + edges_y**2)\n",
        "        return edges\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract deep features\n",
        "        deep_features = self.backbone(x)\n",
        "        deep_features = self.perceptual_head(deep_features)\n",
        "\n",
        "        # Extract edge features\n",
        "        edge_features = self.extract_edge_features(x)\n",
        "        edge_features = F.adaptive_avg_pool2d(edge_features, (8, 8))\n",
        "        edge_features = edge_features.flatten(1)\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat([deep_features, edge_features], dim=1)\n",
        "\n",
        "        return combined"
      ],
      "metadata": {
        "id": "aLDciP974Zbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Siamese network for image similarity detection with focus on structural changes\n",
        "    while being robust to lighting and compression artifacts.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "\n",
        "        self.feature_extractor = PerceptualFeatureExtractor(pretrained=pretrained)\n",
        "\n",
        "        # Calculate feature dimension (64 from perceptual head + 64 from edge features)\n",
        "        feature_dim = 64 + 64  # 128 total\n",
        "\n",
        "        # Similarity head\n",
        "        self.similarity_head = nn.Sequential(\n",
        "            nn.Linear(feature_dim * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        \"\"\"Forward pass for one image\"\"\"\n",
        "        return self.feature_extractor(x)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        \"\"\"Forward pass for image pair\"\"\"\n",
        "        features1 = self.forward_once(img1)\n",
        "        features2 = self.forward_once(img2)\n",
        "\n",
        "        # Concatenate features\n",
        "        combined = torch.cat([features1, features2], dim=1)\n",
        "\n",
        "        # Compute similarity score\n",
        "        similarity = self.similarity_head(combined)\n",
        "\n",
        "        return similarity"
      ],
      "metadata": {
        "id": "vyB-h0Xa4cj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageSimilarityDetector:\n",
        "    \"\"\"\n",
        "    Main class for image similarity detection with preprocessing and inference pipeline.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path=None, device=None):\n",
        "        if device is None:\n",
        "            self.device = torch.device('cpu')  # CPU-only as specified\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        self.model = SiameseNetwork(pretrained=True)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            self.load_model(model_path)\n",
        "\n",
        "        # Image preprocessing pipeline\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Threshold for similarity decision\n",
        "        self.similarity_threshold = 0.5\n",
        "\n",
        "    def preprocess_image(self, image_input: Union[str, np.ndarray, Image.Image]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Preprocess image for model input with additional noise reduction.\n",
        "        \"\"\"\n",
        "        # Handle different input types\n",
        "        if isinstance(image_input, str):\n",
        "            image = Image.open(image_input).convert('RGB')\n",
        "        elif isinstance(image_input, np.ndarray):\n",
        "            if image_input.shape[2] == 3:  # BGR to RGB\n",
        "                image_input = cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB)\n",
        "            image = Image.fromarray(image_input)\n",
        "        elif isinstance(image_input, Image.Image):\n",
        "            image = image_input.convert('RGB')\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported image input type\")\n",
        "\n",
        "        # Apply noise reduction (optional - helps with compression artifacts)\n",
        "        img_array = np.array(image)\n",
        "        img_array = cv2.bilateralFilter(img_array, 9, 75, 75)\n",
        "        image = Image.fromarray(img_array)\n",
        "\n",
        "        # Apply transforms\n",
        "        tensor = self.transform(image)\n",
        "        return tensor.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    def extract_structural_features(self, image_tensor: torch.Tensor) -> dict:\n",
        "        \"\"\"\n",
        "        Extract additional structural features for robustness analysis.\n",
        "        \"\"\"\n",
        "        # Convert to numpy for OpenCV operations\n",
        "        img_np = image_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "        img_np = (img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n",
        "        img_np = np.clip(img_np, 0, 255).astype(np.uint8)\n",
        "\n",
        "        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Extract keypoints and descriptors\n",
        "        orb = cv2.ORB_create(nfeatures=100)\n",
        "        keypoints, descriptors = orb.detectAndCompute(gray, None)\n",
        "\n",
        "        # Compute image statistics\n",
        "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
        "        hist = hist.flatten() / hist.sum()  # Normalize\n",
        "\n",
        "        return {\n",
        "            'keypoints': len(keypoints) if keypoints else 0,\n",
        "            'descriptors': descriptors,\n",
        "            'histogram': hist\n",
        "        }\n",
        "\n",
        "    def compute_similarity(self, img1_path: Union[str, np.ndarray, Image.Image],\n",
        "                          img2_path: Union[str, np.ndarray, Image.Image],\n",
        "                          use_structural_validation=True) -> Tuple[bool, float, dict]:\n",
        "        \"\"\"\n",
        "        Main inference function to determine if two images are similar.\n",
        "\n",
        "        Returns:\n",
        "            - bool: True if images are similar, False if different\n",
        "            - float: Raw similarity score (0-1)\n",
        "            - dict: Additional analysis metrics\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Preprocess images\n",
        "            img1_tensor = self.preprocess_image(img1_path).to(self.device)\n",
        "            img2_tensor = self.preprocess_image(img2_path).to(self.device)\n",
        "\n",
        "            # Get model prediction\n",
        "            similarity_score = self.model(img1_tensor, img2_tensor).item()\n",
        "\n",
        "            # Additional structural validation\n",
        "            analysis_metrics = {}\n",
        "            if use_structural_validation:\n",
        "                features1 = self.extract_structural_features(img1_tensor)\n",
        "                features2 = self.extract_structural_features(img2_tensor)\n",
        "\n",
        "                # Keypoint difference analysis\n",
        "                keypoint_diff = abs(features1['keypoints'] - features2['keypoints'])\n",
        "                keypoint_ratio = keypoint_diff / max(features1['keypoints'], features2['keypoints'], 1)\n",
        "\n",
        "                # Histogram correlation (lighting robustness check)\n",
        "                hist_corr = cv2.compareHist(features1['histogram'], features2['histogram'], cv2.HISTCMP_CORREL)\n",
        "\n",
        "                analysis_metrics = {\n",
        "                    'keypoint_difference': keypoint_diff,\n",
        "                    'keypoint_ratio': keypoint_ratio,\n",
        "                    'histogram_correlation': hist_corr,\n",
        "                    'raw_similarity': similarity_score\n",
        "                }\n",
        "\n",
        "                # Adjust threshold based on histogram correlation\n",
        "                # If histograms are very similar (lighting similar), use stricter threshold\n",
        "                if hist_corr > 0.8:\n",
        "                    adjusted_threshold = self.similarity_threshold + 0.1\n",
        "                else:\n",
        "                    adjusted_threshold = self.similarity_threshold\n",
        "\n",
        "                is_similar = similarity_score > adjusted_threshold\n",
        "            else:\n",
        "                is_similar = similarity_score > self.similarity_threshold\n",
        "                analysis_metrics = {'raw_similarity': similarity_score}\n",
        "\n",
        "            return is_similar, similarity_score, analysis_metrics\n",
        "\n",
        "    def set_threshold(self, threshold: float):\n",
        "        \"\"\"Set similarity threshold (0-1)\"\"\"\n",
        "        self.similarity_threshold = max(0.0, min(1.0, threshold))\n",
        "\n",
        "    def save_model(self, path: str):\n",
        "        \"\"\"Save trained model\"\"\"\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'threshold': self.similarity_threshold\n",
        "        }, path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path: str):\n",
        "        \"\"\"Load trained model\"\"\"\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if 'threshold' in checkpoint:\n",
        "            self.similarity_threshold = checkpoint['threshold']\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "    def train_model(self, train_dataloader, val_dataloader=None, epochs=20, lr=0.001):\n",
        "        \"\"\"\n",
        "        Training function for the model.\n",
        "        Expects dataloader with (img1, img2, label) where label=1 for similar, 0 for different.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        criterion = nn.BCELoss()\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for batch_idx, (img1, img2, labels) in enumerate(train_dataloader):\n",
        "                img1, img2, labels = img1.to(self.device), img2.to(self.device), labels.float().to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(img1, img2).squeeze()\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                if batch_idx % 10 == 0:\n",
        "                    print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "            avg_train_loss = train_loss / len(train_dataloader)\n",
        "            train_acc = 100. * train_correct / train_total\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}:')\n",
        "            print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "\n",
        "            # Validation\n",
        "            if val_dataloader:\n",
        "                val_loss, val_acc = self.validate(val_dataloader, criterion)\n",
        "                print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    self.save_model('best_model.pth')\n",
        "\n",
        "            print('-' * 50)\n",
        "\n",
        "    def validate(self, val_dataloader, criterion):\n",
        "        \"\"\"Validation function\"\"\"\n",
        "        self.model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for img1, img2, labels in val_dataloader:\n",
        "                img1, img2, labels = img1.to(self.device), img2.to(self.device), labels.float().to(self.device)\n",
        "                outputs = self.model(img1, img2).squeeze()\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        self.model.train()\n",
        "        return val_loss / len(val_dataloader), 100. * val_correct / val_total"
      ],
      "metadata": {
        "id": "RzL4T6rO46lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for easy integration\n",
        "def create_detector(model_path=None):\n",
        "    \"\"\"Factory function to create detector instance\"\"\"\n",
        "    return ImageSimilarityDetector(model_path=model_path)\n",
        "\n",
        "def detect_image_similarity(img1_path, img2_path, model_path=None, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Convenience function for one-off similarity detection.\n",
        "\n",
        "    Args:\n",
        "        img1_path: Path to first image or image array/PIL Image\n",
        "        img2_path: Path to second image or image array/PIL Image\n",
        "        model_path: Path to trained model (optional)\n",
        "        threshold: Similarity threshold (default: 0.5)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if images are similar, False if different\n",
        "    \"\"\"\n",
        "    detector = ImageSimilarityDetector(model_path=model_path)\n",
        "    detector.set_threshold(threshold)\n",
        "    is_similar, score, metrics = detector.compute_similarity(img1_path, img2_path)\n",
        "    return is_similar"
      ],
      "metadata": {
        "id": "j2Np6r4u5A8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "detector = ImageSimilarityDetector()\n",
        "\n",
        "# Example: Compare two images\n",
        "# is_similar, score, metrics = detector.compute_similarity('image1.jpg', 'image2.jpg')\n",
        "# print(f\"Images are similar: {is_similar}\")\n",
        "# print(f\"Similarity score: {score:.3f}\")\n",
        "# print(f\"Analysis metrics: {metrics}\")\n",
        "\n",
        "print(\"Image Similarity Detector initialized successfully!\")\n",
        "print(\"Model is ready for CPU inference.\")\n",
        "print(\"\\nKey features:\")\n",
        "print(\"- Robust to lighting changes and compression artifacts\")\n",
        "print(\"- Detects meaningful structural changes\")\n",
        "print(\"- CPU-optimized for production use\")\n",
        "print(\"- Easy integration with existing Python systems\")"
      ],
      "metadata": {
        "id": "MMs6fJml5D20"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}